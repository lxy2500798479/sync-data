#!/usr/bin/env python3
"""
Qdrant æŸ¥è¯¢æµ‹è¯•å·¥å…·
ç”¨äºæµ‹è¯•å‘é‡æœç´¢å’ŒåŒ¹é…åŠŸèƒ½
"""

import os
import sys
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from sync_data.qdrant_manager import QdrantManager
from sync_data.embedding_service import LocalEmbeddingService

# æ¨èæ¨¡å‹åˆ—è¡¨
RECOMMENDED_MODELS = {
    'best': 'BAAI/bge-large-zh-v1.5',
    'tencent': 'tencent/Youtu-Embedding',
    'balanced': 'shibing624/text2vec-base-chinese',
    'light': 'paraphrase-multilingual-MiniLM-L12-v2'
}


def interactive_search():
    """äº¤äº’å¼æœç´¢"""
    print("=" * 60)
    print("ğŸ” Qdrant å‘é‡æœç´¢æµ‹è¯•å·¥å…·")
    print("=" * 60)
    print()
    
    # åˆå§‹åŒ–æœåŠ¡
    print("ğŸ”— è¿æ¥ Qdrant...")
    qdrant = QdrantManager()
    
    # å…ˆé€‰æ‹©é›†åˆï¼Œæ ¹æ®å‘é‡ç»´åº¦è‡ªåŠ¨é€‰æ‹©æ¨¡å‹
    print("\nğŸ“‹ å¯ç”¨é›†åˆ:")
    collections = qdrant.list_collections()
    if not collections:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é›†åˆ")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œæ•°æ®è¿ç§»: python main.py --all")
        return
    
    for i, col in enumerate(collections):
        vector_size = col.get('vector_size', 'unknown')
        print(f"  {i+1}. {col['name']} ({col.get('points_count', 0)} ä¸ªå‘é‡, {vector_size}ç»´)")
    
    col_choice = input("\nè¯·é€‰æ‹©é›†åˆ (1-N): ").strip()
    try:
        collection_idx = int(col_choice) - 1
        if 0 <= collection_idx < len(collections):
            collection_name = collections[collection_idx]['name']
            vector_size = collections[collection_idx].get('vector_size', 'unknown')
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            return
    except ValueError:
        print("âŒ æ— æ•ˆè¾“å…¥")
        return
    
    # æ ¹æ®å‘é‡ç»´åº¦è‡ªåŠ¨é€‰æ‹©æ¨¡å‹
    print(f"\nğŸ“Š æ£€æµ‹åˆ°é›†åˆå‘é‡ç»´åº¦: {vector_size}")
    if isinstance(vector_size, int):
        if vector_size == 1024:
            model_name = RECOMMENDED_MODELS['best']
            print(f"ğŸ’¡ è‡ªåŠ¨é€‰æ‹©: æœ€ä½³ä¸­æ–‡æ¨¡å‹ ({model_name})")
        elif vector_size == 768:
            model_name = RECOMMENDED_MODELS['balanced']
            print(f"ğŸ’¡ è‡ªåŠ¨é€‰æ‹©: å¹³è¡¡æ¨¡å‹ ({model_name})")
        elif vector_size == 384:
            model_name = RECOMMENDED_MODELS['light']
            print(f"ğŸ’¡ è‡ªåŠ¨é€‰æ‹©: è½»é‡çº§æ¨¡å‹ ({model_name})")
        else:
            print(f"âš ï¸  æœªçŸ¥çš„å‘é‡ç»´åº¦ {vector_size}ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
            model_name = RECOMMENDED_MODELS['balanced']
    else:
        print("âš ï¸  æ— æ³•æ£€æµ‹å‘é‡ç»´åº¦ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
        model_name = RECOMMENDED_MODELS['balanced']
    
    print(f"\nâœ… ä½¿ç”¨çš„é›†åˆ: {collection_name}")
    print(f"âœ… ä½¿ç”¨çš„æ¨¡å‹: {model_name}")
    print("ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½ï¼‰...")
    
    embedding = LocalEmbeddingService(model_name)
    
    # å¼€å§‹æœç´¢å¾ªç¯
    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹æœç´¢ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
    print("=" * 60)
    
    while True:
        question = input("\nè¯·è¾“å…¥é—®é¢˜: ").strip()
        
        if question.lower() in ['quit', 'exit', 'q']:
            print("ğŸ‘‹ å†è§!")
            break
        
        if not question:
            continue
        
        print(f"\nğŸ” æœç´¢: \"{question}\"")
        print("-" * 60)
        
        try:
            # ç”Ÿæˆå‘é‡
            vector = embedding.encode_single(question)
            
            # æœç´¢
            results = qdrant.search(
                collection_name=collection_name,
                query_vector=vector,
                limit=5,
                score_threshold=0.5
            )
            
            if not results:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ç»“æœ")
                continue
            
            print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:\n")
            
            for i, result in enumerate(results, 1):
                payload = result.payload
                score = result.score
                
                # ä» metadata ä¸­è·å–æ•°æ®
                metadata = payload.get('metadata', {})
                content = payload.get('content', 'N/A')
                
                print(f"{i}. ç›¸ä¼¼åº¦: {score:.4f}")
                print(f"   é—®é¢˜: {content}")
                print(f"   æ„å›¾: {metadata.get('intentName', 'N/A')}")
                
                # æ˜¾ç¤ºç­”æ¡ˆï¼ˆå–ç¬¬ä¸€ä¸ªï¼‰
                answers = metadata.get('answers', [])
                if answers and len(answers) > 0:
                    answer = answers[0]
                    if isinstance(answer, dict):
                        ans_content = answer.get('content', {})
                        if isinstance(ans_content, dict):
                            text = ans_content.get('text', '')
                            if isinstance(text, str):
                                preview = text[:100] + ('...' if len(text) > 100 else '')
                                print(f"   ç­”æ¡ˆ: {preview}")
                            elif isinstance(text, list) and len(text) > 0:
                                preview = text[0][:100] + ('...' if len(text[0]) > 100 else '')
                                print(f"   ç­”æ¡ˆ: {preview}")
                
                print()
        
        except Exception as e:
            print(f"âŒ æœç´¢å¤±è´¥: {e}")


def quick_test(question: str, company_id: str, model_name: str = None):
    """å¿«é€Ÿæµ‹è¯•"""
    if model_name is None:
        model_name = RECOMMENDED_MODELS['balanced']
    
    print(f"ğŸ” æœç´¢é—®é¢˜: \"{question}\"")
    print(f"ğŸ¢ å…¬å¸ID: {company_id}")
    print(f"ğŸ§  æ¨¡å‹: {model_name}")
    print("-" * 60)
    
    try:
        # åˆå§‹åŒ–
        qdrant = QdrantManager()
        embedding = LocalEmbeddingService(model_name)
        
        # é›†åˆåç§°
        collection_name = f"kb_{company_id}"
        
        # ç”Ÿæˆå‘é‡
        vector = embedding.encode_single(question)
        
        # æœç´¢
        results = qdrant.search(
            collection_name=collection_name,
            query_vector=vector,
            limit=3,
            score_threshold=0.5
        )
        
        if not results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ç»“æœ")
            return
        
        print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:\n")
        
        for i, result in enumerate(results, 1):
            payload = result.payload
            score = result.score
            
            # ä» metadata ä¸­è·å–æ•°æ®
            metadata = payload.get('metadata', {})
            content = payload.get('content', 'N/A')
            
            print(f"{i}. ç›¸ä¼¼åº¦: {score:.4f}")
            print(f"   é—®é¢˜: {content}")
            print(f"   æ„å›¾: {metadata.get('intentName', 'N/A')}")
            
            # æ˜¾ç¤ºç­”æ¡ˆ
            answers = metadata.get('answers', [])
            if answers and len(answers) > 0:
                answer = answers[0]
                if isinstance(answer, dict):
                    ans_content = answer.get('content', {})
                    if isinstance(ans_content, dict):
                        text = ans_content.get('text', '')
                        if isinstance(text, str):
                            print(f"   ç­”æ¡ˆ: {text[:200]}...")
                        elif isinstance(text, list) and len(text) > 0:
                            print(f"   ç­”æ¡ˆ: {text[0][:200]}...")
            
            print()
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) > 1:
        # å‘½ä»¤è¡Œæ¨¡å¼
        if len(sys.argv) < 3:
            print("ç”¨æ³•:")
            print("  python test_query.py                    # äº¤äº’å¼æœç´¢")
            print("  python test_query.py <é—®é¢˜> <å…¬å¸ID>    # å¿«é€Ÿæµ‹è¯•")
            return
        
        question = sys.argv[1]
        company_id = sys.argv[2]
        model_name = sys.argv[3] if len(sys.argv) > 3 else None
        
        quick_test(question, company_id, model_name)
    else:
        # äº¤äº’å¼æ¨¡å¼
        interactive_search()


if __name__ == "__main__":
    main()

