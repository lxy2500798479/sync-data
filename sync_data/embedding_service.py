"""
æœ¬åœ°åµŒå…¥æœåŠ¡æ¨¡å—
æ”¯æŒå¤šç§å…è´¹çš„ä¸­æ–‡åµŒå…¥æ¨¡å‹
"""

from sentence_transformers import SentenceTransformer
import numpy as np
from typing import List, Dict, Any
import torch
import os


# æ¨èçš„å…è´¹ä¸­æ–‡æ¨¡å‹é…ç½®
RECOMMENDED_MODELS = {
    "bge-large-zh-v1.5": {
        "model_name": "BAAI/bge-large-zh-v1.5",
        "dimensions": 1024,
        "description": "ç™¾åº¦å¼€æºï¼Œä¸­æ–‡æ•ˆæœå¾ˆå¥½ï¼Œæ¨èä½¿ç”¨",
        "size": "~1.3GB",
        "performance": "é«˜"
    },
    "youtu-embedding": {
        "model_name": "tencent/Youtu-Embedding",
        "dimensions": 1024,
        "description": "è…¾è®¯ä¼˜å›¾ï¼Œä¸­æ–‡æ•ˆæœä¼˜ç§€ï¼Œä¼ä¸šçº§",
        "size": "~1.3GB",
        "performance": "é«˜"
    },
    "text2vec-large-chinese": {
        "model_name": "shibing624/text2vec-large-chinese", 
        "dimensions": 1024,
        "description": "ä¸“é—¨é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–",
        "size": "~1.3GB",
        "performance": "é«˜"
    },
    "text2vec-base-chinese": {
        "model_name": "shibing624/text2vec-base-chinese",
        "dimensions": 768, 
        "description": "è¾ƒå°çš„æ¨¡å‹ï¼Œé€Ÿåº¦å¿«",
        "size": "~400MB",
        "performance": "ä¸­"
    },
    "paraphrase-multilingual": {
        "model_name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        "dimensions": 384,
        "description": "å¤šè¯­è¨€æ”¯æŒï¼ŒåŒ…å«ä¸­æ–‡ï¼Œæœ€è½»é‡",
        "size": "~470MB",
        "performance": "ä¸­ä½"
    }
}


class LocalEmbeddingService:
    """æœ¬åœ°åµŒå…¥æœåŠ¡"""
    
    def __init__(self, model_name: str = "BAAI/bge-large-zh-v1.5"):
        """
        åˆå§‹åŒ–æœ¬åœ°åµŒå…¥æœåŠ¡
        
        Args:
            model_name: åµŒå…¥æ¨¡å‹åç§°ï¼Œæ”¯æŒä»¥ä¸‹æ¨¡å‹ï¼š
                - BAAI/bge-large-zh-v1.5 (æ¨èï¼Œä¸­æ–‡æ•ˆæœæœ€å¥½)
                - shibing624/text2vec-base-chinese (é€Ÿåº¦å¿«)
                - sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 (è½»é‡çº§)
        """
        print(f"ğŸš€ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
        
        # è®¾ç½®è®¾å¤‡
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        if self.device == 'cpu':
            print("âš ï¸  ä½¿ç”¨CPUè¿è¡Œï¼Œé€Ÿåº¦å¯èƒ½è¾ƒæ…¢ã€‚å¦‚æœ‰GPUï¼Œè¯·å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch")
        
        # è®¾ç½®ç¼“å­˜ç›®å½•
        cache_dir = os.getenv('HF_CACHE_DIR', './models_cache')
        os.makedirs(cache_dir, exist_ok=True)
        
        try:
            # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ safetensors é¿å… torch.load æ¼æ´ï¼‰
            self.model = SentenceTransformer(
                model_name, 
                device=self.device,
                cache_folder=cache_dir,
                trust_remote_code=True,
                # å¼ºåˆ¶ä½¿ç”¨ safetensors æ ¼å¼
                use_auth_token=None
            )
            self.model_name = model_name
            self.dimensions = self.model.get_sentence_embedding_dimension()
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            print(f"   æ¨¡å‹åç§°: {self.model_name}")
            print(f"   å‘é‡ç»´åº¦: {self.dimensions}")
            print(f"   ç¼“å­˜ç›®å½•: {cache_dir}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ å»ºè®®å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š")
            print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   2. å°è¯•ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹")
            print("   3. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°")
            raise
    
    def encode_single(self, text: str) -> List[float]:
        """ç¼–ç å•ä¸ªæ–‡æœ¬"""
        if not text or not text.strip():
            print("âš ï¸ å‘ç°ç©ºæ–‡æœ¬ï¼Œä½¿ç”¨é›¶å‘é‡")
            return [0.0] * self.dimensions
            
        try:
            embedding = self.model.encode(
                text, 
                convert_to_tensor=False, 
                normalize_embeddings=True,
                show_progress_bar=False
            )
            return embedding.tolist()
        except Exception as e:
            print(f"âŒ æ–‡æœ¬ç¼–ç å¤±è´¥: {e}")
            print(f"   é—®é¢˜æ–‡æœ¬: {text[:100]}...")
            return [0.0] * self.dimensions
    
    def encode_batch(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:
        """æ‰¹é‡ç¼–ç æ–‡æœ¬"""
        if not texts:
            return []
        
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡ç¼–ç  {len(texts)} ä¸ªæ–‡æœ¬...")
        
        # é¢„å¤„ç†æ–‡æœ¬
        processed_texts = []
        for i, text in enumerate(texts):
            # ç¡®ä¿textæ˜¯å­—ç¬¦ä¸²ç±»å‹
            if not isinstance(text, str):
                print(f"âš ï¸ ç¬¬ {i+1} ä¸ªæ–‡æœ¬ä¸æ˜¯å­—ç¬¦ä¸²ç±»å‹: {type(text)} - {text}")
                if isinstance(text, (list, tuple)) and text:
                    # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå°è¯•è·å–ç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²å…ƒç´ 
                    text_str = ""
                    for item in text:
                        if isinstance(item, str) and item.strip():
                            text_str = item.strip()
                            break
                    if text_str:
                        processed_texts.append(text_str)
                    else:
                        processed_texts.append("ç©ºæ–‡æœ¬")
                else:
                    processed_texts.append(str(text) if text else "ç©ºæ–‡æœ¬")
            elif not text or not text.strip():
                print(f"âš ï¸ ç¬¬ {i+1} ä¸ªæ–‡æœ¬ä¸ºç©ºï¼Œä½¿ç”¨å ä½ç¬¦")
                processed_texts.append("ç©ºæ–‡æœ¬")
            else:
                processed_texts.append(text.strip())
        
        try:
            # æ‰¹é‡ç¼–ç 
            embeddings = self.model.encode(
                processed_texts, 
                batch_size=batch_size,
                convert_to_tensor=False,
                normalize_embeddings=True,
                show_progress_bar=True,
                device=self.device
            )
            
            print(f"âœ… æ‰¹é‡ç¼–ç å®Œæˆï¼ç”Ÿæˆäº† {len(embeddings)} ä¸ªå‘é‡")
            return embeddings.tolist()
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡ç¼–ç å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°å•ä¸ªç¼–ç æ¨¡å¼...")
            
            # å›é€€åˆ°å•ä¸ªç¼–ç 
            results = []
            for i, text in enumerate(processed_texts):
                try:
                    result = self.encode_single(text)
                    results.append(result)
                    if (i + 1) % 10 == 0:
                        print(f"   å·²å¤„ç† {i+1}/{len(processed_texts)} ä¸ªæ–‡æœ¬")
                except Exception as single_error:
                    print(f"âŒ ç¬¬ {i+1} ä¸ªæ–‡æœ¬ç¼–ç å¤±è´¥: {single_error}")
                    results.append([0.0] * self.dimensions)
            
            return results
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "dimensions": self.dimensions,
            "device": self.device,
            "model_type": "local_sentence_transformer"
        }
    
    @staticmethod
    def list_available_models() -> Dict[str, Dict[str, Any]]:
        """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹"""
        return RECOMMENDED_MODELS
    
    @staticmethod
    def recommend_model(gpu_memory_gb: float = 0) -> str:
        """æ ¹æ®ç¡¬ä»¶é…ç½®æ¨èæ¨¡å‹"""
        if gpu_memory_gb >= 8:
            return "BAAI/bge-large-zh-v1.5"  # æˆ– "tencent/Youtu-Embedding"
        elif gpu_memory_gb >= 4:
            return "shibing624/text2vec-base-chinese"
        else:
            return "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    
    def calculate_vector_quality(self, vector: List[float]) -> float:
        """è®¡ç®—å‘é‡è´¨é‡åˆ†æ•°"""
        if not vector or len(vector) != self.dimensions:
            return 0.0
        
        # åŸºäºæ–¹å·®çš„è´¨é‡è¯„ä¼°
        vector_array = np.array(vector)
        mean_val = np.mean(vector_array)
        variance = np.var(vector_array)
        
        # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
        quality_score = min(variance * 10, 1.0)
        return float(quality_score)
